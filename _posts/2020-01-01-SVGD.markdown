---
layout: post
title:  "Stein Variational Gradient Descent"
date:   2020-01-06 6:00:00 -0400
categories: steins-method, stein-variational-gradient-descent, bayesian-inference
author: bhairav
blurb: "Stein Variational Gradient Descent is a powerful, non-parameteric Bayesian Inference algorithm, based on a tool called Stein's Method. In this guide, we work through the mathematics and fundamentals to understand Stein's Method, working our way to Kernelized Stein Discrepancy and, finally, Stein Variational Gradient Descent. We take Reinforcement Learning as an application area, and view SVGD from the lens of gradient flow, bringing new insights and applications to the SVGD toolbox."
feedback: true
---

This guide is thanks to a many different people, all of whom took their time to give feedback, write reviews, and provide their own insights to the curriculum.

Special thanks to Cinjon Resnick, who patiently waited for months as I procrastinated the completion of this guide, as well as Professor Qiang Liu, who took the time to help shape the curriculum. 

Thank you to Calvin Woo, Sanyam Kapoor, Thomas Pinder, and Swapneel Mehta for useful contributions to this guide, as well as countless insights during our discussions.

A special thanks to the many outside guests who offered to provide their time, including Dilin Wang, Tongzheng Ren, and Haoran Tang.

Finally, thank you to all my fellow students who attended the recitations and provided valuable feedback.

<div class="deps-graph">
<iframe class="deps" src="/assets/svgd-deps.svg" width="200"></iframe>
<div>TODO: Concepts used in SVGD. Click to navigate.</div>
</div>

# Why

Stein's Method is a powerful statistical method, one that is at the disposal (and the focus) of many statisticians today. Recently, Stein's Method has made its way into machine learning and has already proved to be a fruitful research area. Stein's Method has deep connections to many machine learning problems of interest, and by the end of this guide, you should be able to understand the relevant mathematics behind this powerful tool.   

<br />

# 1 Introduction + Basics Behind Kernelized Stein Discrepancy
  **Motivation**: Before jumping into all the math and methodology, we have to be able to  understand the basics of what’s going on. Most importantly, we will review the basics of measure theory and reproducing kernel hilbert spaces. Measure theory allows us to understand the notion of discrepancy measures between distributions, which we will use later on to quantify the difference between two arbitrary distributions of interest. Our other topic, Reproducing Kernel Hilbert Spaces (RKHS), will serve as the connection between measure theory and a practical machine learning algorithm. With RKHS, we will be able to define and optimize intractable measures which previously, were only useful for theoretical analysis or a restrictive class of functions. These two together set the foundation for defining a tractable Kernelized Stein Discrepancy, which serves as the driving factor behind Stein Variational Gradient Descent. 


  **Topics**:

  1. Measure Theory
  2. Kernels
  3. Reproducing Kernel Hilbert Space
  4. Machine Learning Basics

  **Notes**: In this class, with notes in [Colab](https://colab.research.google.com/drive/1x3bgKtYWaYRTV1VGaf0bKRyQ_qxNZpjh) or [PDF](/assets/svgd_notes/week01.pdf), we went over the basic mathematical concepts we will need throughout the rest of the curriculum.

  **Required Reading**:

  1. [Reproducing Kernel Hilbert Spaces Tutorial, Section 1 - 3](http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf)
  2. [A gentle introduction to Measure Theory (Chandalia)](https://www.win.tue.nl/~rvhassel/Onderwijs/Old-Onderwijs/2DE08-1011/ConTeXt-OWN-FA-201209-Bib/Literature/sigma-algebra/gc_06_measure_theory.pdf)
  
  **Optional Reading**:

  1. [Slides on RKHS from Arthur Gretton](http://mlss.tuebingen.mpg.de/2015/slides/gretton/part_1.pdf)
  2. [CS231n's Numpy and Python Tutorial](http://cs231n.github.io/python-numpy-tutorial/)
  3. [CS229's Linear Algebra Refresher](http://cs229.stanford.edu/section/cs229-linalg.pdf)
  4. [Xavier Bourret Sicotte's Blog on Kernels and Feature Maps](https://xavierbourretsicotte.github.io/Kernel_feature_map.html)

  **Questions**:

  1. "However, Cauchy sequences are not the same as convergent sequences", but a property of Cauchy sequences is that they are bounded. What's the difference?

  <details><summary>Solution</summary>
    <p>
      Convergent sequences have a limit, but Cauchy sequences are only required to be bounded. But what exactly does bounded mean? Here's a proof that shows that they are bounded, which might shed some light on the definition itself:

      a. There exists $N$ such that $|a_n - a_m| < 1 \quad \forall m, n \geq N$ (Property of Cauchy Sequence iterates getting closer)

      b. $\implies \forall n \geq N, |a_n - a_N| < 1$ 

      c. $a_n \in (a_N - 1, a_N + 1) \forall n \geq N$. ($n \geq N$ is bounded)

      d. Since the sequence is $n < N$ is finite (since $N$ is finite), it is also bounded.

      Therefore the Cauchy sequence $\{ a_n \}$ is bounded $\square$
    </p>
  </details>
  
  2. " The open interval (0, 1) is not complete whereas the closed interval [0, 1] is complete". Why? Can we use this example to get a intuitive definition of **complete**?

  <details><summary>Solution</summary>
    <p>
      Intuitively, a space is complete if there are no "points missing" from it (inside or at the boundary). For instance, the set of rational numbers is not complete, because e.g. $\sqrt(2)$ is "missing" from it, even though one can construct a Cauchy sequence of rational numbers that converges to it. ([Wikipedia: Complete Metric Space](https://en.wikipedia.org/wiki/Complete_metric_space))
    </p>
  </details>

  3. Explain the difference between a Banach and Hilbert Space. Is every Hilbert space a Banach space?

  <details><summary>Solution</summary>
    <p>
      A Banach space is a vector space in which each vector has a non-negative length, or norm, and in which every Cauchy sequence converges to a point of the space. Also known as complete normed linear space.

      A Hilbert space is a Banach space with inner product, which defines the norm.
    </p>
  </details>

  4. In Machine Learning, kernels can be thought of as a "dot product" (a kind of similarity score) in high-dimensional space. Why would this be useful? Given a feature map, do we always have a corresponding kernel? Given any kernel, can we always explicitly write out the elements of the corresponding feature map?

  <details><summary>Solution</summary>
    <p>
      Kernels (and the corresponding kernel trick) allow us to compute similarities in high-dimensional space without explicitly writing out and computing the dot product. 

      However, not ever feature map corresponds to a kernel; there are certain properties a kernel must have, and not every feature map imbues it with those properties.

      Likewise, given a kernel, it may be the case that we can never write out (explicitly) the corresponding feature map. A good example of this is the popular **exponential kernel**.
    </p>
  </details>

  5. What is the goal of supervised learning? In a high-dimensional, supervised setting, what makes this hard?

  <details><summary>Solution</summary>
    <p>
      We want to learn a distribution $p$ that fits the data generating distribution $q$, while still being flexible enough to do learning and inference tractibly / efficiently. In high dimensions, the bias-variance tradeoff in model selection can hit us hard, along with another whole host of problems.
    </p>
  </details>

  6. Assume that we just need the log-likeihood in many machine learning tasks (not a completely false statement...) so that we can compute  KL(q||p) , and iteratively fit our model  p  to the underlying, generating data distribution  q . Why is this already too large of an assumption ("We assume that we have the ability to calculate the log-likelihood under the model that we specify")?

  <details><summary>Solution</summary>
    <p>
      The dreaded normalization constant! Most models we see will give an unnormalized likelihood, and the normalization constant (which we will see in a few weeks, often denoted as $Z$) is intractable to compute. We need the normalization constant to bring a probability function to a probability **density**function.
    </p>
  </details>

  7. What is the use of Monte-Carlo methods in machine learning?

  <details><summary>Solution</summary>
    <p>
      They are a way to estimate quantities in the presence of complex, many-random-variable situations. They do so by repeatedly generating (via simulation) instances from which they estimate the quantities. 
    </p>
  </details>

  8. Explain the reproducing property in your own words.

  <details><summary>Solution</summary>
    <p>
      [Sanyam Kapoor's](TODO) answer from our class was: "Every feature map is a linear combination of the full Hilbert space weighted by the kernel evaluations."
    </p>
  </details>
  
<br />

# 2 Stein's Method
  **Motivation**: Most of the theory we will see in this curriculum builds off the general theoretical framework of Stein’s Method, a tool to obtain bounds on distances between distributions. In Machine Learning (as we shall later see), distances between distributions can be used to quantify how well (or poorly) a model is at approximating a certain distribution of interest. We shall start from Stein’s Identity and Operator, while explaining their theoretical significance and working through some proofs to get an understanding of some terms (Stein’s Method, Stein’s Discrepancy) we’ll see in the coming weeks. Lastly, we will discuss why Stein’s Method has historically been a theoretical tool, and hint at how ideas from Week 1 (particularly RKHS) can be used in combination with Stein’s Method to build the tractable discrepancy measure at the center of Week 3’s discussion.

  **Topics**:

  1. Stein's Method
  2. The Stein Operator
  3. Stein Equation
  4. Stein's Identity

  **Notes**: In this class, with notes in [Colab](https://colab.research.google.com/drive/1HqHSP9x01te7e33-zDR00vAsPX_M19h2) or [PDF](/assets/svgd_notes/week02.pdf), we discussed the theoretical concepts behind Stein's method, and discussed different ways to interpret the core ideas.

  **Required Reading**:

  1. [Section 2 of Kernelized Stein Discrepancy](https://arxiv.org/abs/1602.03253)
  2. [Stein's Method on Wikipedia](https://en.wikipedia.org/wiki/Stein%27s_method)
  
  **Optional Reading**:

  1. [A Short History of Stein's Method](https://arxiv.org/abs/1404.1392).
  
  **Questions**:

   
  1. What determines the choice of kernel in KSD?

    <details><summary>Solution</summary>
      <p>
          Since KSD requires an RKHS for optimization, the kernel _must_ be positive definite. However, whenever given a positive definite kernel $K$, we can always build an associated RKHS as follows. 

          If we take $H$ as the Hilbert space of functions $f: \mathcal{X} \rightarrow \mathbf{R}$ defined on some set $\mathcal{X}$ with some inner product $\langle \cdot, \cdot \rangle_H defined on $H$, then we can define the _evaluation functional_ $e_x: H \rightarrow \mathbf{R}$ as $f \rightarrow e_x(f) = f(x).

          Using the above definitions, our space $H$ is an RKHS iff the evaluation functionals are continuous. As we saw in the notes, we call the given kernel $K$ a _reproducing kernel_ if:

          1. $K(x, \cdot), \; \forall x \in \mathcal{X}$ 
          2. $\langle f, K_x \rangle = f(x) \; \forall f \in H, \forall x \in \mathcal{X}$.

          Thus, every reproducing kernel $K$ induces a unique RKHS given the kernel is positive definite. 

          Excitingly, in the context of machine learning, positive definite kernels themselves can be defined in terms of inner products. Therefore, we can generate arbitrary kernels and RKHS with some feature map $\Phi: \mathcal{X} \rightarrow \mathcal{F}$ where feature space $\mathcal{F}$ is a Hilbert space with some inner product $\langle \cdot, \cdot \rangle. 
      </p>
    </details>

  2. Show condition six (in the Kernelized Stein Discrepancy paper) holds through integration by parts.   
    <details><summary>Solution</summary>
      <p>
        The proof via divergence theorem is inside of the paper (assuming a compact space $\mathcal{X}$), so we review the proof via IBP here.

      </p>
    </details>

<br />

# 3 Kernelized Stein Discrepancy
  **Motivation**: The main theoretical meat comes from a single 2016 paper titled Kernelized Stein Discrepancy (KSD). KSD takes the powerful Stein’s Identity, and uses RKHS theory to define a tractable discrepancy between a ground truth distribution and samples from an arbitrary one. Most importantly, KSD defines a discrepancy function that does not involve calculating the normalizing constant, allowing it to be much more widely applicable in practical tasks. We will discuss the difference between likelihood-free and likelihood-based methods in machine learning, how this normalization constant proves to be problematic in machine learning, and how KSD allows us to sidestep this issue with a new, tractable discrepancy. KSD will serve as the launch pad for the algorithm at the focus of this curriculum, Stein Variational Gradient Descent. 

  **Topics**:

  1. A Stein Discrepancy
  2. Goodness of Fit
  3. Tractable Optimization of the Stein Discrepancy

  **Notes**: In this class, with notes in [Colab](https://colab.research.google.com/drive/1V7zpm9U3TCjIM9DxeRWo6IEypDkZObrH) or [PDF](/assets/svgd_notes/week03.pdf), we worked through the Kernelized Stein Discrepancy paper, focusing on the optimization and use cases of such a method.

  **Required Reading**:

  1. [A Short Introduction to Kernelized Stein Discrepancy](http://www.cs.dartmouth.edu/~qliu/PDF/ksd_short.pdf)
  2. [ICML 2015 Slides on KSD](https://www.cs.dartmouth.edu/~qliu/PDF/slides_ksd_icml2015.pdf)
  3. [Kernelized Stein Discrepancy](https://arxiv.org/abs/1602.03253)
  4. [What is Maximum Mean Discrepancy?](https://stats.stackexchange.com/questions/276497/maximum-mean-discrepancy-distance-distribution)
  
  **Optional Reading**:

  1. [Measuring Sample Quality with Stein’s Method (Similar concurrent result from Lester Mackey)](https://arxiv.org/abs/1506.03039)

  **Questions**:

  1. Imagine the test function f is a linear combination of a finite number of features. What is this particular maximization’s relation to RKHS?

<br />

# 4 Stein Variational Gradient Descent
  **Motivation**:  Stein Variational Gradient Descent (SVGD) is a popular, non-parametric Bayesian Inference algorithm that’s been applied to Variational Inference, Reinforcement Learning, GANs, and much more. This week, we study the algorithm in its entirety, building off of last week’s work on KSD, and seeing how viewing KSD from a KL-Divergence-minimization lens induces a powerful, practical algorithm. We discuss the benefits of SVGD over other similar approximators, and look at a practical implementation of the algorithm.

  **Topics**:

  1. Stein Variational Gradient Descent
  2. Implementing the Algorithm
  
  **Notes**: In this class, with notes in [Colab](https://colab.research.google.com/drive/0B2rVTvobCLlWNEY4SENKdG1OQ3c) or [PDF](/assets/svgd_notes/week04.pdf), we go over the core paper, Stein Variational Gradient Descent. At the end of the notes, we provide link to implementations in a variety of different languages. 

  **Required Reading**:

  1. [SVGD Slides](https://www.cs.dartmouth.edu/~qliu/PDF/steinslides16.pdf)
  2. [SVGD Paper](https://arxiv.org/abs/1608.04471).
  3. [Sanyam Kapoor's great notebook on Stein Gradients](https://www.sanyamkapoor.com/machine-learning/stein-gradient/).
  
  **Optional Reading**:

  1. [SVGD: Theory and Applications](https://www.cs.dartmouth.edu/~qliu/PDF/svgd_aabi2016.pdf).
  2. [Learning to Sample with Amortized SVGD](https://arxiv.org/abs/1707.06626).
  
  **Questions**:

  1. Compare and contrast the method shown here and MCMC. What are some advantages MCMC still has over SVGD?

  2. Prove that the discrepancy in Equation 3 of the Stein Variational Gradient Descent Paper only equals 0 when $p$ and $q$ are equal.

  3. Implement SVGD in your favorite language (see the notes for links to different implementations). Then, let’s take a look at the role of the kernel in SVGD: (a) Remove the repulsive kernel term and observe how particles collapse to modes. (b) Remove the kernel’s contribution in the first term. What happens?	

  4. What 

<br />

# 5 Stein in Reinforcement Learning
  **Motivation**: One of the most exciting use cases of SVGD is in reinforcement learning, due to its connection to maximum entropy reinforcement learning. This week, we study two key techniques in reinforcement learning that use SVGD as the underlying mechanism. In reinforcement learning, the target distribution is not known, so we derive gradient updates to our parameters using policy gradients. As we derive the gradient estimators in the maximum-entropy framework of reinforcement learning, we will start to see what benefits SVGD-based methods have. In particular, we will focus on the explore-exploit tradeoff, as well as normalization constants for intractable distributions, and see how SVGD helps us get around complicated problems regarding both. 

  **Topics**:

  1. Reinforcement Learning
  2. Explore vs. Exploit
  3. Maximum Entropy Reinforcement Learning

  **Notes**:  In this class, with notes in [Colab](https://colab.research.google.com/drive/178X8BgGrUmPaRTLulL_ETUKaBf-MfrgS) or [PDF](/assets/svgd_notes/week06.pdf), we look at the application area of _Reinforcement Learning_, and see how the diversity induced by SVGD (and its connection to maxmimum entropy reinforcement learning) generates strongly-exploring policies. 

  **Required Reading**:

  1. [Stein Variational Policy Gradient](https://arxiv.org/abs/1704.02399)
  2. [Reinforcement Learning with Energy-Based Policies](https://arxiv.org/abs/1702.08165)
  
  **Optional Reading**:

  1. [A Long Peek into Reinforcement Learning](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html)
  2. [Learning to Draw Samples with Amortized SVGD - Same as W4](https://arxiv.org/abs/1707.06626)
  3. [Soft Q-Learning BAIR Blogpost](https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/)
  4. [Learning Self-Imitating Diverse Policies (an improved SVPG)](https://arxiv.org/abs/1805.10309)
  5. [Bayesian MAML](https://arxiv.org/abs/1806.03836)
  
  **Questions**:

  1. What are some of the issues with using the RBF kernel when comparing RL policies? Is parameter space appropriate for comparing policies?

  2. Hypothesize a new heuristic that adapts the bandwidth of the RBF kernel. In general, what are some issues that can arise with SVPG (and inherently, SVGD) in large spaces?

  3. In SVPG, the introduction of a prior (and priors in RL) is one active area of research. To incorporate priors in this framework, what "space" does the prior need to be over? 
   
   4. With the code implementation linked in the notes (or, your own), ablate on the architecture of each SVPG particle. What types of behavioral differences do you see in the different policies as you increase or decrease? Try adding a second layer instead; for example, how does a 2-layer, 200 neuron-per-layer network compare to a single-layer, 400 neuron particle?
     
<br />


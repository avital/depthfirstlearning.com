---
layout: post
title:  "Stein Variational Gradient Descent"
date:   2020-01-06 6:00:00 -0400
categories: steins-method, stein-variational-gradient-descent, bayesian-inference
author: bhairav
blurb: "Stein Variational Gradient Descent is a powerful, non-parameteric Bayesian Inference algorithm, based on a tool called Stein's Method. In this guide, we work through the mathematics and fundamentals to understand Stein's Method, working our way to Kernelized Stein Discrepancy and, finally, Stein Variational Gradient Descent. We take Reinforcement Learning as an application area, and view SVGD from the lens of gradient flow, bringing new insights and applications to the SVGD toolbox."
feedback: true
---

This guide is thanks to a many different people, all of whom took their time to give feedback, write reviews, and provide their own insights to the curriculum.

Special thanks to Cinjon Resnick, who patiently waited for months as I procrastinated the completion of this guide, as well as Professor Qiang Liu, who took the time to help shape the curriculum. 

Thank you to Calvin Woo, Sanyam Kapoor, Thomas Pinder, and Swapneel Mehta for useful contributions to this guide, as well as countless insights during our discussions.

A special thanks to the many outside guests who offered to provide their time, including Dilin Wang, Tongzheng Ren, and Haoran Tang.

Finally, thank you to all my fellow students who attended the recitations and provided valuable feedback.

<div class="deps-graph">
<iframe class="deps" src="/assets/svgd-deps.svg" width="200"></iframe>
<div>TODO: Concepts used in SVGD. Click to navigate.</div>
</div>

# Why

Stein's Method is a powerful statistical method, one that is at the disposal (and the focus) of many statisticians today. Recently, Stein's Method has made its way into machine learning and has already proved to be a fruitful research area. Stein's Method has deep connections to many machine learning problems of interest, and by the end of this guide, you should be able to understand the relevant mathematics behind this powerful tool.   

<br />

# 1 Introduction + Basics Behind Kernelized Stein Discrepancy
  **Motivation**: Before jumping into all the math and methodology, we have to be able to  understand the basics of what’s going on. Most importantly, we will review the basics of measure theory and reproducing kernel hilbert spaces. Measure theory allows us to understand the notion of discrepancy measures between distributions, which we will use later on to quantify the difference between two arbitrary distributions of interest. Our other topic, Reproducing Kernel Hilbert Spaces (RKHS), will serve as the connection between measure theory and a practical machine learning algorithm. With RKHS, we will be able to define and optimize intractable measures which previously, were only useful for theoretical analysis or a restrictive class of functions. These two together set the foundation for defining a tractable Kernelized Stein Discrepancy, which serves as the driving factor behind Stein Variational Gradient Descent. 


  **Topics**:

  1. Measure Theory
  2. Kernels
  3. Reproducing Kernel Hilbert Space
  4. Machine Learning Basics

  **Notes**: In this class, with notes in [Colab](https://colab.research.google.com/drive/1x3bgKtYWaYRTV1VGaf0bKRyQ_qxNZpjh) or [PDF - Coming Soon](#), we went over the basic mathematical concepts we will need throughout the rest of the curriculum.

  **Required Reading**:

  1. [Reproducing Kernel Hilbert Spaces Tutorial, Section 1 - 3](http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf)
  2. [A gentle introduction to Measure Theory (Chandalia)](https://www.win.tue.nl/~rvhassel/Onderwijs/Old-Onderwijs/2DE08-1011/ConTeXt-OWN-FA-201209-Bib/Literature/sigma-algebra/gc_06_measure_theory.pdf)
  
  **Optional Reading**:

  1. [Slides on RKHS from Arthur Gretton](http://mlss.tuebingen.mpg.de/2015/slides/gretton/part_1.pdf)
  2. [CS231n's Numpy and Python Tutorial](http://cs231n.github.io/python-numpy-tutorial/)
  3. [CS229's Linear Algebra Refresher](http://cs229.stanford.edu/section/cs229-linalg.pdf)
  4. [Xavier Bourret Sicotte's Blog on Kernels and Feature Maps](https://xavierbourretsicotte.github.io/Kernel_feature_map.html)

  **Questions**:

  1. What are the conditions that determine whether a measure space is a probability space?

  2. Explain the reproducing property in your own words.

  3. Given the means and standard deviations of N i.i.d gaussians (in a diagonal covariance matrix), write a python function that generates samples from that joint distribution and another that calculates the log-likelihood of any given sample.

  4. Explain what a [score function](https://en.wikipedia.org/wiki/Score_(statistics)) (\nabla_\theta \log p_\theta(x)) tells us and how it might be useful in machine learning. Implement the calculation of the score function of a multivariate normal distribution.
 

<br />

# 2 Stein's Method
  **Motivation**: Most of the theory we will see in this curriculum builds off the general theoretical framework of Stein’s Method, a tool to obtain bounds on distances between distributions. In Machine Learning (as we shall later see), distances between distributions can be used to quantify how well (or poorly) a model is at approximating a certain distribution of interest. We shall start from Stein’s Identity and Operator, while explaining their theoretical significance and working through some proofs to get an understanding of some terms (Stein’s Method, Stein’s Discrepancy) we’ll see in the coming weeks. Lastly, we will discuss why Stein’s Method has historically been a theoretical tool, and hint at how ideas from Week 1 (particularly RKHS) can be used in combination with Stein’s Method to build the tractable discrepancy measure at the center of Week 3’s discussion.

  **Topics**:

  1. Stein's Method
  2. The Stein Operator
  3. Stein Equation
  4. Stein's Identity

  **Notes**: In this class, with notes in [Colab](https://colab.research.google.com/drive/1HqHSP9x01te7e33-zDR00vAsPX_M19h2) or [PDF - Coming Soon](#), we discussed the theoretical concepts behind Stein's method, and discussed different ways to interpret the core ideas.

  **Required Reading**:

  1. [Section 2 of Kernelized Stein Discrepancy](https://arxiv.org/abs/1602.03253)
  2. [Stein's Method on Wikipedia](https://en.wikipedia.org/wiki/Stein%27s_method)
  
  **Optional Reading**:

  1. [A Short History of Stein's Method](https://arxiv.org/abs/1404.1392).
  
  **Questions**:

  1. Could you prove Lemma 2.2 from the KSD paper without looking at their proof? There is another illustrative way to prove this lemma - can you find it?
   
  2. What determines the choice of kernel in KSD? Are there kernels you can think of that don’t meet the necessary requirements.

  3. What’s the difference of the score function introduced in KSD in Section 1 and 2 and the score function we see in last week’s questions?

  4. Show condition six (in the Kernelized Stein Discrepancy paper) holds through integration by parts.  Optionally, show this with the divergence theorem.
  

<br />

# 3 Kernelized Stein Discrepancy
  **Motivation**: The main theoretical meat comes from a single 2016 paper titled Kernelized Stein Discrepancy (KSD). KSD takes the powerful Stein’s Identity, and uses RKHS theory to define a tractable discrepancy between a ground truth distribution and samples from an arbitrary one. Most importantly, KSD defines a discrepancy function that does not involve calculating the normalizing constant, allowing it to be much more widely applicable in practical tasks. We will discuss the difference between likelihood-free and likelihood-based methods in machine learning, how this normalization constant proves to be problematic in machine learning, and how KSD allows us to sidestep this issue with a new, tractable discrepancy. KSD will serve as the launch pad for the algorithm at the focus of this curriculum, Stein Variational Gradient Descent. 

  **Topics**:

  1. A Stein Discrepancy
  2. Goodness of Fit
  3. Tractable Optimization of the Stein Discrepancy

  **Notes**: In this class, with notes in [Colab](https://colab.research.google.com/drive/1V7zpm9U3TCjIM9DxeRWo6IEypDkZObrH) or [PDF - Coming Soon](#), we worked through the Kernelized Stein Discrepancy paper, focusing on the optimization and use cases of such a method.

  **Required Reading**:

  1. [A Short Introduction to Kernelized Stein Discrepancy](http://www.cs.dartmouth.edu/~qliu/PDF/ksd_short.pdf)
  2. [ICML 2015 Slides on KSD](https://www.cs.dartmouth.edu/~qliu/PDF/slides_ksd_icml2015.pdf)
  3. [Kernelized Stein Discrepancy](https://arxiv.org/abs/1602.03253)
  4. [What is Maximum Mean Discrepancy?](https://stats.stackexchange.com/questions/276497/maximum-mean-discrepancy-distance-distribution)
  
  **Optional Reading**:

  1. [Measuring Sample Quality with Stein’s Method (Similar concurrent result from Lester Mackey)](https://arxiv.org/abs/1506.03039)

  **Questions**:

  1. Explicitly show why the “note” in the proof of Theorem 3.6 is true. Are there any cases where this doesn’t hold? Why or why not?

  2. In Section 4 of KSD, we can approximate the U-statistic with the V-statistic`. Why is that? And where is the bias incurred in this approximation?

  3. Solve the maximization (Equation 2 of KSD) when the test function f is a linear combination of a finite number of features. What is this particular maximization’s relation to RKHS?

<br />

# 4 Stein Variational Gradient Descent
  **Motivation**:  Stein Variational Gradient Descent (SVGD) is a popular, non-parametric Bayesian Inference algorithm that’s been applied to Variational Inference, Reinforcement Learning, GANs, and much more. This week, we study the algorithm in its entirety, building off of last week’s work on KSD, and seeing how viewing KSD from a KL-Divergence-minimization lens induces a powerful, practical algorithm. We discuss the benefits of SVGD over other similar approximators, and look at a practical implementation of the algorithm.

  **Topics**:

  1. Stein Variational Gradient Descent
  2. Implementing the Algorithm
  
  **Notes**: In this class, with notes in [Colab](https://colab.research.google.com/drive/0B2rVTvobCLlWNEY4SENKdG1OQ3c) or [PDF - Coming Soon](#), we go over the core paper, Stein Variational Gradient Descent. At the end of the notes, we provide link to implementations in a variety of different languages. 

  **Required Reading**:

  1. [SVGD Slides](https://www.cs.dartmouth.edu/~qliu/PDF/steinslides16.pdf)
  2. [SVGD Paper](https://arxiv.org/abs/1608.04471).
  3. [Sanyam Kapoor's great notebook on Stein Gradients](https://www.sanyamkapoor.com/machine-learning/stein-gradient/).
  
  **Optional Reading**:

  1. [SVGD: Theory and Applications](https://www.cs.dartmouth.edu/~qliu/PDF/svgd_aabi2016.pdf).
  2. [Learning to Sample with Amortized SVGD](https://arxiv.org/abs/1707.06626).
  
  **Questions**:

  1. Compare and contrast the method shown here and MCMC. What are some advantages MCMC still has over SVGD?

  2. Prove that the discrepancy in Equation 3 of the Stein Variational Gradient Descent Paper only equals 0 when $p$ and $q$ are equal.

  3. Implement SVGD in your favorite language (see the notes for links to different implementations). Then, let’s take a look at the role of the kernel in SVGD: (a) Remove the repulsive kernel term and observe how particles collapse to modes. (b) Remove the kernel’s contribution in the first term. What happens?	

<br />

# 5 Stein in Reinforcement Learning
  **Motivation**: One of the most exciting use cases of SVGD is in reinforcement learning, due to its connection to maximum entropy reinforcement learning. This week, we study two key techniques in reinforcement learning that use SVGD as the underlying mechanism. In reinforcement learning, the target distribution is not known, so we derive gradient updates to our parameters using policy gradients. As we derive the gradient estimators in the maximum-entropy framework of reinforcement learning, we will start to see what benefits SVGD-based methods have. In particular, we will focus on the explore-exploit tradeoff, as well as normalization constants for intractable distributions, and see how SVGD helps us get around complicated problems regarding both. 

  **Topics**:

  1. Reinforcement Learning
  2. Explore vs. Exploit
  3. Maximum Entropy Reinforcement Learning

  **Notes**:  In this class, with notes in [Colab](https://colab.research.google.com/drive/178X8BgGrUmPaRTLulL_ETUKaBf-MfrgS) or [PDF - Coming Soon](#), we look at the application area of _Reinforcement Learning_, and see how the diversity induced by SVGD (and its connection to maxmimum entropy reinforcement learning) generates strongly-exploring policies. 

  **Required Reading**:

  1. [Stein Variational Policy Gradient](https://arxiv.org/abs/1704.02399)
  2. [Reinforcement Learning with Energy-Based Policies](https://arxiv.org/abs/1702.08165)
  
  **Optional Reading**:

  1. [A Long Peek into Reinforcement Learning](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html)
  2. [Learning to Draw Samples with Amortized SVGD - Same as W4](https://arxiv.org/abs/1707.06626)
  3. [Soft Q-Learning BAIR Blogpost](https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/)
  4. [Learning Self-Imitating Diverse Policies (an improved SVPG)](https://arxiv.org/abs/1805.10309)
  5. [Bayesian MAML](https://arxiv.org/abs/1806.03836)
  
  **Questions**:

  1. What are some of the issues with using the RBF kernel when comparing RL policies? Is parameter space appropriate for comparing policies?

  2. Hypothesize a new heuristic that adapts the bandwidth of the RBF kernel. In general, what are some issues that can arise with SVPG (and inherently, SVGD) in large spaces?

  3. In SVPG, the introduction of a prior (and priors in RL) is one active area of research. To incorporate priors in this framework, what "space" does the prior need to be over? 
   
   4. With the code implementation linked in the notes (or, your own), ablate on the architecture of each SVPG particle. What types of behavioral differences do you see in the different policies as you increase or decrease? Try adding a second layer instead; for example, how does a 2-layer, 200 neuron-per-layer network compare to a single-layer, 400 neuron particle?
     
<br />

